{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Chapter 3: Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## 3.1 Research Strategy \n",
    "\n",
    "Success Metrics\n",
    "Approach: See main submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.2 Data Collection and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import Libraries and Dependencies\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Dependencies for data cleaning, analysis and manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "from scipy import stats\n",
    "from math import floor,ceil\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#Dependencies for Data Visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "# import scikitplot as skplt\n",
    "\n",
    "#Dependencies for Feature Extraction and Engineering\n",
    "\n",
    "# SKLEARN is installed from scikit-learn as sklearn is deperecated\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "### Dependencies for five (5) Model Creation \n",
    "\n",
    "#Dependencies for Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Dependencies for Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Dependencies for Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Dependencies for Support Vector Machines\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Dependencies for K-Nearest Neighbours (KNN)\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Dependencies for Model Evaluation\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, accuracy_score, classification_report, roc_curve,auc, f1_score, precision_score, recall_score, roc_auc_score\n",
    "import sklearn.metrics as metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Data collection from data source\n",
    "\n",
    "german_credit_data = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\",sep=\" \",header=None)\n",
    "\n",
    "column_headers = [\"Status of existing checking account\",\"Duration in month\",\"Credit history\",\\\n",
    "         \"Purpose\",\"Credit amount\",\"Savings account/bonds\",\"Present employment since\",\\\n",
    "         \"Installment rate in percentage of disposable income\",\"Personal status and sex\",\\\n",
    "         \"Other debtors / guarantors\",\"Present residence since\",\"Property\",\"Age in years\",\\\n",
    "        \"Other installment plans\",\"Housing\",\"Number of existing credits at this bank\",\\\n",
    "        \"Job\",\"Number of people being liable to provide maintenance for\",\"Telephone\",\"foreign worker\",\"Cost Matrix(Risk)\"]\n",
    "\n",
    "german_credit_data.columns = column_headers\n",
    "\n",
    "#Save as CSV file\n",
    "german_credit_data.to_csv(\"germancreditdata.csv\",index=False)\n",
    "\n",
    "#Read data from CSV file\n",
    "ger_credit_df = pd.read_csv(\"germancreditdata.csv\")\n",
    "\n",
    "ger_credit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Target data distribution\n",
    "\n",
    "ger_credit_df['Cost Matrix(Risk)'].astype(int).plot.hist(color='forestgreen').set_xlabel('Target or dependent variable: 1 or 2');\n",
    "\n",
    "count = ger_credit_df['Cost Matrix(Risk)'].value_counts()\n",
    "good_risk = count[1]\n",
    "bad_risk = count[2]\n",
    "\n",
    "print(\"There are {} loans repaid on time (Good Risk, Cost Matrix(Risk) =1) and {} loans defaulted (Bad Risk, Cost Matrix(Risk)=2) in the dataset\".format(good_risk, bad_risk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data description\n",
    "\n",
    "::: German Credit Data:::\n",
    "\n",
    "1000 entries make up the dataset, which also includes 20 independent variables (13 categorical, 7 numerical), and 1 target variable created by Prof. Hofmann. Each entry in this collection reflects a person who accepts a bank credit. Depending on a set of factors, each person is categorised as either a good or bad credit risk.\n",
    "\n",
    "::: USA Credit Data :::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# The current CSV file collected contains keys like \"A14, A61\" for each of the columns\n",
    "# UCI provides a description of each of this keys. \n",
    "# To proceed we need to map this these keys/entries to their respective descriptions by creating \n",
    "# a dictionary with a key-value pair for each feature. The key represents current entry, while values represent description\n",
    "# and using the .map() iterative method. The map() method creates a new array populated with \n",
    "# the results of calling a provided function on every element in the calling array.\n",
    "\n",
    "\n",
    "\n",
    "#Key-Value Pair for 13 categorical features\n",
    "Status_of_existing_checking_account={'A14':\"no checking account\",'A11':\"<0 DM\", 'A12': \"0 <= <200 DM\",'A13':\">= 200 DM \"}\n",
    "Credit_history={\"A34\":\"critical account\",\"A33\":\"delay in paying off\",\"A32\":\"existing credits paid back duly till now\",\"A31\":\"all credits at this bank paid back duly\",\"A30\":\"no credits taken\"}\n",
    "Purpose={\"A40\" : \"car (new)\", \"A41\" : \"car (used)\", \"A42\" : \"furniture/equipment\", \"A43\" :\"radio/television\" , \"A44\" : \"domestic appliances\", \"A45\" : \"repairs\", \"A46\" : \"education\", 'A47' : 'vacation','A48' : 'retraining','A49' : 'business','A410' : 'others'}\n",
    "Saving_account={\"A65\" : \"no savings account\",\"A61\" :\"<100 DM\",\"A62\" : \"100 <= <500 DM\",\"A63\" :\"500 <= < 1000 DM\", \"A64\" :\">= 1000 DM\"}\n",
    "Present_employment={'A75':\">=7 years\", 'A74':\"4<= <7 years\",  'A73':\"1<= < 4 years\", 'A72':\"<1 years\",'A71':\"unemployed\"}\n",
    "Personal_status_and_sex={ 'A95':\"female:single\",'A94':\"male:married/widowed\",'A93':\"male:single\", 'A92':\"female:divorced/separated/married\", 'A91':\"male:divorced/separated\"}\n",
    "Other_debtors_guarantors={'A101':\"none\", 'A102':\"co-applicant\", 'A103':\"guarantor\"}\n",
    "Property={'A121':\"real estate\", 'A122':\"savings agreement/life insurance\", 'A123':\"car or other\", 'A124':\"unknown / no property\"}\n",
    "Other_installment_plans={'A143':\"none\", 'A142':\"store\", 'A141':\"bank\"}\n",
    "Housing={'A153':\"for free\", 'A152':\"own\", 'A151':\"rent\"}\n",
    "Job={'A174':\"management/ highly qualified employee\", 'A173':\"skilled employee / official\", 'A172':\"unskilled - resident\", 'A171':\"unemployed/ unskilled  - non-resident\"}\n",
    "Telephone={'A192':\"yes\", 'A191':\"none\"}\n",
    "foreign_worker={'A201':\"yes\", 'A202':\"no\"}\n",
    "risk={1:\"Good Risk\", 2:\"Bad Risk\"}\n",
    "\n",
    "#Using Map function to replace values in the columns\n",
    "ger_credit_df[\"Status of existing checking account\"]=ger_credit_df[\"Status of existing checking account\"].map(Status_of_existing_checking_account)\n",
    "ger_credit_df[\"Credit history\"]=ger_credit_df[\"Credit history\"].map(Credit_history)\n",
    "ger_credit_df[\"Purpose\"]=ger_credit_df[\"Purpose\"].map(Purpose)\n",
    "ger_credit_df[\"Savings account/bonds\"]=ger_credit_df[\"Savings account/bonds\"].map(Saving_account)\n",
    "ger_credit_df[\"Present employment since\"]=ger_credit_df[\"Present employment since\"].map(Present_employment)\n",
    "ger_credit_df[\"Personal status and sex\"]=ger_credit_df[\"Personal status and sex\"].map(Personal_status_and_sex)\n",
    "ger_credit_df[\"Other debtors / guarantors\"]=ger_credit_df[\"Other debtors / guarantors\"].map(Other_debtors_guarantors)\n",
    "ger_credit_df[\"Property\"]=ger_credit_df[\"Property\"].map(Property)\n",
    "ger_credit_df[\"Other installment plans\"]=ger_credit_df[\"Other installment plans\"].map(Other_installment_plans)\n",
    "ger_credit_df[\"Housing\"]=ger_credit_df[\"Housing\"].map(Housing)\n",
    "ger_credit_df[\"Job\"]=ger_credit_df[\"Job\"].map(Job)\n",
    "ger_credit_df[\"Telephone\"]=ger_credit_df[\"Telephone\"].map(Telephone)\n",
    "ger_credit_df[\"foreign worker\"]=ger_credit_df[\"foreign worker\"].map(foreign_worker)\n",
    "ger_credit_df[\"Cost Matrix(Risk)\"]=ger_credit_df[\"Cost Matrix(Risk)\"].map(risk)\n",
    "\n",
    "\n",
    "ger_credit_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Rename Column \n",
    "\n",
    "ger_credit_df.rename(columns = {'Cost Matrix(Risk)':'Credit Risk'}, inplace = True)\n",
    "ger_credit_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Save a copy of the dataset\n",
    "backup_ger_credit_df = ger_credit_df.copy()\n",
    "backup_ger_credit_df.head(5)\n",
    "# ger_credit_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Data Overview\n",
    "ger_credit_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ger_credit_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for null values \n",
    "ger_credit_df.isna().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Statistical view of numerical variables\n",
    "ger_credit_df[[\"Credit amount\",\\\n",
    "               \"Duration in month\",\\\n",
    "               \"Installment rate in percentage of disposable income\",\\\n",
    "               \"Age in years\",\\\n",
    "               \"Present residence since\",\\\n",
    "               \"Number of existing credits at this bank\",\\\n",
    "               \"Number of people being liable to provide maintenance for\"\n",
    "               \n",
    "              ]].describe()\n",
    "\n",
    "\n",
    "# .to_csv(\"stats_credit2.csv\", index=False)\n",
    "\n",
    "# ger_credit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Checking for and removal of outliers\n",
    "\n",
    "sns.set()\n",
    "f, axes = plt.subplots(1, 3,figsize=(15,5))\n",
    "sns.boxplot(y=ger_credit_df[\"Credit amount\"],x=ger_credit_df[\"Credit Risk\"],orient='v' , ax=axes[0],palette=[\"#800080\",\"#808000\"]) #box plot\n",
    "sns.boxplot(y=ger_credit_df[\"Duration in month\"],x=ger_credit_df[\"Credit Risk\"], orient='v' , ax=axes[1],palette=[\"#800080\",\"#808000\"]) #box plot\n",
    "sns.boxplot(y=ger_credit_df[\"Installment rate in percentage of disposable income\"],x=ger_credit_df[\"Credit Risk\"], orient='v' , ax=axes[2],palette=[\"#800080\",\"#808000\"]) #box plot\n",
    "\n",
    "plt.show()\n",
    "\n",
    "sns.set()\n",
    "f, axes = plt.subplots(1, 3,figsize=(15,5))\n",
    "sns.boxplot(y=ger_credit_df[\"Age in years\"],x=ger_credit_df[\"Credit Risk\"],orient='v' , ax=axes[0],palette=[\"#800080\",\"#808000\"]) #box plot\n",
    "sns.boxplot(y=ger_credit_df[\"Present residence since\"],x=ger_credit_df[\"Credit Risk\"], orient='v' , ax=axes[1],palette=[\"#800080\",\"#808000\"]) #box plot\n",
    "sns.boxplot(y=ger_credit_df[\"Number of existing credits at this bank\"],x=ger_credit_df[\"Credit Risk\"], orient='v' , ax=axes[2],palette=[\"#800080\",\"#808000\"]) #box plot\n",
    "\n",
    "plt.show()\n",
    "\n",
    "sns.set()\n",
    "f, axes = plt.subplots(1, 1,figsize=(15,5))\n",
    "sns.boxplot(y=ger_credit_df[\"Number of people being liable to provide maintenance for\"],x=ger_credit_df[\"Credit Risk\"],orient='v',palette=[\"#800080\",\"#808000\"]) #box plot\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "## Replacement of outliers using IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Removing outliers with the IQR method\n",
    "\n",
    "class OutlierDetector:\n",
    "    def __init__(self, variables, ger_credit_df):\n",
    "        self.variables = variables\n",
    "        self.ger_credit_df = ger_credit_df\n",
    "\n",
    "    def detect_outliers(self):\n",
    "        for v in self.variables:\n",
    "            subset_0 = self.ger_credit_df[self.ger_credit_df[\"Credit Risk\"] == \"Good Risk\"]\n",
    "            subset_1 = self.ger_credit_df[self.ger_credit_df[\"Credit Risk\"] == \"Bad Risk\"]\n",
    "\n",
    "            q75,q25 = np.percentile(subset_0.loc[:, v],[75, 25])\n",
    "            interval_q = q75 - q25\n",
    "            max_value = q75 + (1.5 * interval_q)\n",
    "            min_value = q25 - (1.5 * interval_q)\n",
    "\n",
    "            for i in range(1, len(self.ger_credit_df)):\n",
    "                if (self.ger_credit_df.loc[i, v] < min_value) | (self.ger_credit_df.loc[i, v] > max_value):\n",
    "                    self.ger_credit_df.loc[i, v] = statistics.mean(self.ger_credit_df[v])\n",
    "\n",
    "            q75,q25 = np.percentile(subset_1.loc[:, v],[75, 25])\n",
    "            interval_q = q75 - q25\n",
    "            max_value = q75 + (1.5 * interval_q)\n",
    "            min_value = q25 - (1.5 * interval_q)\n",
    "\n",
    "            for i in range(1, len(self.ger_credit_df)):\n",
    "                if (self.ger_credit_df.loc[i, v] < min_value) | (self.ger_credit_df.loc[i, v] > max_value):\n",
    "                    self.ger_credit_df.loc[i, v] = statistics.mean(self.ger_credit_df[v])\n",
    "\n",
    "# Instantiate the class and run the function\n",
    "variables = [\"Credit amount\",\"Duration in month\",\n",
    "              \"Installment rate in percentage of disposable income\",\n",
    "              \"Age in years\",\"Present residence since\",\n",
    "              \"Number of existing credits at this bank\",\n",
    "              \"Number of people being liable to provide maintenance for\"]\n",
    "detector = OutlierDetector(variables, ger_credit_df)\n",
    "detector.detect_outliers()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Test to confirm that outliers have been replaced using the IQR method\n",
    "\n",
    "ger_credit_df[[\"Credit amount\",\\\n",
    "               \"Duration in month\",\\\n",
    "               \"Installment rate in percentage of disposable income\",\\\n",
    "               \"Age in years\",\\\n",
    "               \"Present residence since\",\\\n",
    "               \"Number of existing credits at this bank\",\\\n",
    "               \"Number of people being liable to provide maintenance for\"\n",
    "               \n",
    "              ]].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Correlation Analysis\n",
    "## Closely correlated values selected using P-Values or IV are further\n",
    "## filtered using correlation analysis removed. \n",
    "\n",
    "c_matrix = ger_credit_df.corr(method='pearson')\n",
    "sns.heatmap(c_matrix, annot=True,cmap='PRGn', linewidths=0.3, annot_kws={\"size\":12}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Feature Selection and Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#OOP P-Values\n",
    "#Feature Selection for German Data\n",
    "\n",
    "class StatisticalSignificance:\n",
    "    def __init__(self, ger_credit_df):\n",
    "        self.ger_credit_df = ger_credit_df\n",
    "\n",
    "    def calculate_cat_p_values(self,column_names_cat_stats):\n",
    "        statistical_significance=[]\n",
    "        for attr in column_names_cat_stats:\n",
    "            data_count=pd.crosstab(self.ger_credit_df[attr],self.ger_credit_df[\"Credit Risk\"]).reset_index()\n",
    "            obs=np.asarray(data_count[[\"Bad Risk\",\"Good Risk\"]])\n",
    "            chi2, p, dof, expected = stats.chi2_contingency(obs)\n",
    "            statistical_significance.append([attr,round(p,6)])\n",
    "\n",
    "        statistical_significance=pd.DataFrame(statistical_significance)\n",
    "        statistical_significance.columns=[\"attribute\",\"P-value\"]\n",
    "\n",
    "        return statistical_significance\n",
    "    \n",
    "    def calculate_cont_p_values(self,column_names_cont_stats):\n",
    "        statistical_significance=[]\n",
    "        good_risk_df = self.ger_credit_df[self.ger_credit_df[\"Credit Risk\"]==\"Good Risk\"]\n",
    "        bad_risk_df = self.ger_credit_df[self.ger_credit_df[\"Credit Risk\"]==\"Bad Risk\"]\n",
    "        for attr in column_names_cont_stats:\n",
    "            statistic, p=stats.f_oneway(good_risk_df[attr].values,bad_risk_df[attr].values)\n",
    "            statistical_significance.append([attr,round(p,6)])\n",
    "        statistical_significance=pd.DataFrame(statistical_significance)\n",
    "        statistical_significance.columns=[\"attribute\",\"P-value\"]\n",
    "        return statistical_significance\n",
    "        \n",
    "# Instantiate the class and run the function\n",
    "column_names_cat_stats=[\"Status of existing checking account\",\"Credit history\",\"Purpose\",\\\n",
    "\"Savings account/bonds\",\"Present employment since\",\\\n",
    "\"Personal status and sex\",\\\n",
    "\"Other debtors / guarantors\",\"Property\",\\\n",
    "\"Other installment plans\",\"Housing\",\\\n",
    "\"Job\",\"Telephone\",\"foreign worker\"]\n",
    "\n",
    "column_names_cont_stats=[\"Credit amount\",\"Duration in month\",\n",
    "                         \"Installment rate in percentage of disposable income\",\n",
    "                         \"Age in years\",\"Present residence since\",\n",
    "                         \"Number of existing credits at this bank\",\n",
    "                         \"Number of people being liable to provide maintenance for\"]\n",
    "\n",
    "stat_significance = StatisticalSignificance(ger_credit_df)\n",
    "cat_p_values = stat_significance.calculate_cat_p_values(column_names_cat_stats)\n",
    "cont_p_values = stat_significance.calculate_cont_p_values(column_names_cont_stats)\n",
    "\n",
    "# display(cat_p_values)\n",
    "# display(cont_p_values)\n",
    "\n",
    "p_values_categorical = pd.DataFrame(cat_p_values)\n",
    "p_values_continous = pd.DataFrame(cont_p_values)\n",
    "\n",
    "# Sort P-Values in ascending order\n",
    "p_values_categorical = p_values_categorical.sort_values(by=['P-value'], ascending=True)\n",
    "p_values_continous = p_values_continous.sort_values(by=['P-value'], ascending=True)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#PValues of categorical variables\n",
    "\n",
    "p_values_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# PValues of continous variables\n",
    "\n",
    "p_values_continous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#OneHotEncoding for categorical variables\n",
    "\n",
    "\n",
    "#Drop 6 columns with P-values less than 0.05 statistical significance\n",
    "\n",
    "df_selected = ger_credit_df.drop(columns=['Number of people being liable to provide maintenance for',\\\n",
    "                      'Present residence since',\\\n",
    "                      'Job',\\\n",
    "                      'Telephone',\\\n",
    "                      'Credit amount',\\\n",
    "                      'Number of existing credits at this bank'\n",
    "                                         ])\n",
    "\n",
    "\n",
    "# Create a list holding 11 categorical variables in the dataset\n",
    "\n",
    "col_cat_names=[\"Status of existing checking account\",\"Credit history\",\"Purpose\",\\\n",
    "\"Savings account/bonds\",\"Present employment since\",\\\n",
    "\"Personal status and sex\",\"Other debtors / guarantors\",\"Property\",\"Other installment plans\",\"Housing\",\"foreign worker\"]\n",
    "\n",
    "# Use one-hot encoding to create dummy variables for 11 categorical variables created and 1 target variable\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, df_selected, col_cat_names):\n",
    "        self.df_selected = df_selected\n",
    "        self.col_cat_names = col_cat_names\n",
    "\n",
    "    def one_hot_encode(self):\n",
    "        for attr in self.col_cat_names:\n",
    "            self.df_selected = self.df_selected.merge(pd.get_dummies(self.df_selected[attr], prefix=attr), left_index=True, right_index=True)\n",
    "            self.df_selected.drop(attr, axis=1, inplace=True)\n",
    "            \n",
    "    def replace_value(self):\n",
    "        risk = {\"Good Risk\": 1, \"Bad Risk\": 0}\n",
    "        self.df_selected[\"Credit Risk\"] = self.df_selected[\"Credit Risk\"].map(risk)\n",
    "        \n",
    "\n",
    "\n",
    "preprocessor = Preprocessor(df_selected, col_cat_names)\n",
    "preprocessor.one_hot_encode()\n",
    "preprocessor.replace_value()\n",
    "df_dummies = preprocessor.df_selected\n",
    "df_dummies\n",
    "# df_selected\n",
    "\n",
    "# ger_credit_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Second Data Group for model prediction without \"Credit History\" and \"Foreign Worker\" features\n",
    "\n",
    "df_selected1 = ger_credit_df.drop(columns=['Number of people being liable to provide maintenance for',\\\n",
    "                      'Present residence since','Job', 'Telephone','Credit amount','Number of existing credits at this bank',\\\n",
    "                      'Credit history', 'foreign worker'\n",
    "                                         ])\n",
    "# Second list with names of columns for one-hot encoding of 9 categorical variables for model prediction \n",
    "# without \"Credit History\" and \"Foreign Worker\" features\n",
    "# Final dummy will use 9 categorical variables, and 3 numerical variable and 1 target variable\n",
    "\n",
    "col_cat_names1=[\"Status of existing checking account\",\"Purpose\",\\\n",
    "\"Savings account/bonds\",\"Present employment since\",\\\n",
    "\"Personal status and sex\",\"Other debtors / guarantors\",\"Property\",\"Other installment plans\",\"Housing\"]\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, df_selected1, col_cat_names1):\n",
    "        self.df_selected1 = df_selected1\n",
    "        self.col_cat_names1 = col_cat_names1\n",
    "\n",
    "    def one_hot_encode(self):\n",
    "        for attr in self.col_cat_names1:\n",
    "            self.df_selected1 = self.df_selected1.merge(pd.get_dummies(self.df_selected1[attr], prefix=attr), left_index=True, right_index=True)\n",
    "            self.df_selected1.drop(attr, axis=1, inplace=True)\n",
    "            \n",
    "    def replace_value(self):\n",
    "        risk = {\"Good Risk\": 1, \"Bad Risk\": 0}\n",
    "        self.df_selected1[\"Credit Risk\"] = self.df_selected1[\"Credit Risk\"].map(risk)\n",
    "    \n",
    "\n",
    "preprocessor = Preprocessor(df_selected1, col_cat_names1)\n",
    "preprocessor.one_hot_encode()\n",
    "preprocessor.replace_value()\n",
    "df_dummies1 = preprocessor.df_selected1\n",
    "df_dummies1\n",
    "\n",
    "# df_selected1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Target Variable distribution\n",
    "\n",
    "\n",
    "### OOP Target Data Distribution\n",
    "\n",
    "# Replace \"Good Risk\" and \"Bad Risk\" with \"1\" and \"0\" so it's possible to count occurrence\n",
    "# Use back up data - create a copy from it. \n",
    "\n",
    "df_dist = backup_ger_credit_df.copy()\n",
    "\n",
    "class Replacer:\n",
    "    def __init__(self, df_dist):\n",
    "        self.df_dist = df_dist\n",
    "\n",
    "    def replace_value(self):\n",
    "        risk = {\"Good Risk\": 1, \"Bad Risk\": 0}\n",
    "        self.df_dist[\"Credit Risk\"] = self.df_dist[\"Credit Risk\"].map(risk)\n",
    "        \n",
    "\n",
    "df_replaced = Replacer(df_dist)\n",
    "df_replaced.replace_value()\n",
    "df_dist_replaced = df_replaced.df_dist\n",
    "\n",
    "# Plot graph\n",
    "\n",
    "df_dist_replaced['Credit Risk'].astype(int).plot.hist(color='forestgreen').set_xlabel('Target or dependent variable: 1 or 0');\n",
    "\n",
    "count = df_dist_replaced['Credit Risk'].value_counts()\n",
    "good_risk = count[1]\n",
    "bad_risk = count[0]\n",
    "\n",
    "print(\"There are {} loans repaid on time (Good Risk, Cost Matrix(Risk) =1) and {} loans defaulted (Bad Risk, Cost Matrix(Risk)=0) in the dataset\".format(good_risk, bad_risk))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Spliting dataset into train and test version\n",
    "\n",
    "#Remove Target Variable column, assign resultant dataframe with only indpendent variables to \"x\"\n",
    "x = df_dummies1.drop('Credit Risk', 1).values \n",
    "\n",
    "#Select Target variable column, assign it to \"y\"\n",
    "y = df_dummies1['Credit Risk'].values \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Scaling the dataset\n",
    "\n",
    "scaled_x = StandardScaler()    \n",
    "x_train = scaled_x.fit_transform(x_train)    \n",
    "x_test = scaled_x.transform(x_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Credit Risk Model Development and Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Model Training - Data Group 1\n",
    "l_reg = LogisticRegression (random_state = 0)\n",
    "l_reg.fit(x_train, y_train)\n",
    "\n",
    "# Prediction \n",
    "y_prediction_logreg = l_reg.predict(x_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_logreg = confusion_matrix(y_test, y_prediction_logreg, labels=l_reg.classes_)\n",
    "\n",
    "# Display Confusion Matrix Plot\n",
    "disp_logreg = ConfusionMatrixDisplay(confusion_matrix=cm_logreg, display_labels=l_reg.classes_)\n",
    "disp_logreg.plot()\n",
    "\n",
    "# Metrics\n",
    "TP_logreg = 41\n",
    "TN_logreg  = 175\n",
    "FP_logreg  = 45\n",
    "FN_logreg  = 39\n",
    "\n",
    "ACC_logreg  = round(((TP_logreg  + TN_logreg ) / (TP_logreg + TN_logreg + FP_logreg + FN_logreg))*100,2)\n",
    "print(f\"LogisticRegression Accuracy (%):{ACC_logreg}\")\n",
    "\n",
    "TPR_logreg  = round((TP_logreg / (TP_logreg + FN_logreg))*100,2) \n",
    "print(f\"LogisticRegression Sensitivity, TPR (%):{TPR_logreg}\")\n",
    "\n",
    "SP_logreg  = round((TN_logreg / (FP_logreg + TN_logreg))*100,2)\n",
    "print(f\"LogisticRegression Specificity (%): {SP_logreg}\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Model Training - Data Group 2 - No Credit History or Immigrant tag\n",
    "l_reg = LogisticRegression (random_state = 0)\n",
    "l_reg.fit(x_train, y_train)\n",
    "\n",
    "# Prediction \n",
    "y_prediction_logreg = l_reg.predict(x_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_logreg = confusion_matrix(y_test, y_prediction_logreg, labels=l_reg.classes_)\n",
    "\n",
    "# Display Confusion Matrix Plot\n",
    "disp_logreg = ConfusionMatrixDisplay(confusion_matrix=cm_logreg, display_labels=l_reg.classes_)\n",
    "disp_logreg.plot()\n",
    "\n",
    "# Metrics\n",
    "TP_logreg = 44\n",
    "TN_logreg  = 180\n",
    "FP_logreg  = 42\n",
    "FN_logreg  = 34\n",
    "\n",
    "ACC_logreg  = round(((TP_logreg  + TN_logreg ) / (TP_logreg + TN_logreg + FP_logreg + FN_logreg))*100,2)\n",
    "print(f\"LogisticRegression Accuracy (%):{ACC_logreg}\")\n",
    "\n",
    "TPR_logreg  = round((TP_logreg / (TP_logreg + FN_logreg))*100,2) \n",
    "print(f\"LogisticRegression Sensitivity, TPR (%):{TPR_logreg}\")\n",
    "\n",
    "SP_logreg  = round((TN_logreg / (FP_logreg + TN_logreg))*100,2)\n",
    "print(f\"LogisticRegression Specificity (%): {SP_logreg}\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# XGBoost Model Training - Data Group 1 - complete selected feature\n",
    "\n",
    "xgb = XGBClassifier(max_depth=2,                 # Depth of each tree\n",
    "                            learning_rate=0.1,            # How much to shrink error in each subsequent training. Trade-off with no. estimators.\n",
    "                            n_estimators=50,             # How many trees to use, the more the better, but decrease learning rate if many used.\n",
    "                            verbosity=1,                  # If to show more errors or not.\n",
    "                            objective='binary:logistic',  # Type of target variable.\n",
    "                            booster='gbtree',             # What to boost. Trees in this case.\n",
    "                            n_jobs=2,                    # Parallel jobs to run. Set your processor number.\n",
    "                            gamma=0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n",
    "                            subsample=0.632,              # Subsample ratio. Can set lower\n",
    "                            colsample_bytree=1,           # Subsample ratio of columns when constructing each tree.\n",
    "                            colsample_bylevel=1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest.\n",
    "                            colsample_bynode=1,           # Subsample ratio of columns when constructing each split.\n",
    "                            reg_alpha=1,                  # Regularizer for first fit. alpha = 1, lambda = 0 is LASSO.\n",
    "                            reg_lambda=0,                 # Regularizer for first fit.\n",
    "                            scale_pos_weight=1,           # Balancing of positive and negative weights. G / B\n",
    "                            base_score=0.5,               # Global bias. Set to average of the target rate.\n",
    "                            random_state=0,        # Seed\n",
    "                            #missing=None,                 # How are nulls encoded?\n",
    "                            #tree_method='gpu_hist',       # How to train the trees?\n",
    "                            #gpu_id=0                      # With which GPU? \n",
    "                            )\n",
    "     \n",
    "\n",
    "xgb.fit(x_train, y_train)\n",
    "\n",
    "# Prediction \n",
    "y_prediction_xgb = xgb.predict(x_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_xgb = confusion_matrix(y_test, y_prediction_xgb, labels=xgb.classes_)\n",
    "\n",
    "# Display Confusion Matrix Plot\n",
    "disp_xgb = ConfusionMatrixDisplay(confusion_matrix=cm_xgb, display_labels=xgb.classes_)\n",
    "disp_xgb.plot()\n",
    "\n",
    "# Metrics\n",
    "TP_xgb = 40\n",
    "TN_xgb  = 186\n",
    "FP_xgb  = 46\n",
    "FN_xgb  = 28\n",
    "\n",
    "ACC_xgb  = round(((TP_xgb  + TN_xgb ) / (TP_xgb + TN_xgb + FP_xgb + FN_xgb))*100,2)\n",
    "print(f\"XGBoost Accuracy:{ACC_xgb}%\")\n",
    "\n",
    "TPR_xgb  = round((TP_xgb / (TP_xgb + FN_xgb))*100,2) \n",
    "print(f\"XGBoost Sensitivity, TPR:{TPR_logreg}%\")\n",
    "\n",
    "SP_xgb  = round((TN_xgb / (FP_xgb + TN_xgb))*100,2)\n",
    "print(f\"XGBoost Specificity: {SP_xgb}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# XGBoost Model Training - Data Group 2 - No Credit History or Immigrant tag\n",
    "\n",
    "xgb = XGBClassifier(max_depth=2,                 # Depth of each tree\n",
    "                            learning_rate=0.1,            # How much to shrink error in each subsequent training. Trade-off with no. estimators.\n",
    "                            n_estimators=50,             # How many trees to use, the more the better, but decrease learning rate if many used.\n",
    "                            verbosity=1,                  # If to show more errors or not.\n",
    "                            objective='binary:logistic',  # Type of target variable.\n",
    "                            booster='gbtree',             # What to boost. Trees in this case.\n",
    "                            n_jobs=2,                    # Parallel jobs to run. Set your processor number.\n",
    "                            gamma=0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n",
    "                            subsample=0.632,              # Subsample ratio. Can set lower\n",
    "                            colsample_bytree=1,           # Subsample ratio of columns when constructing each tree.\n",
    "                            colsample_bylevel=1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest.\n",
    "                            colsample_bynode=1,           # Subsample ratio of columns when constructing each split.\n",
    "                            reg_alpha=1,                  # Regularizer for first fit. alpha = 1, lambda = 0 is LASSO.\n",
    "                            reg_lambda=0,                 # Regularizer for first fit.\n",
    "                            scale_pos_weight=1,           # Balancing of positive and negative weights. G / B\n",
    "                            base_score=0.5,               # Global bias. Set to average of the target rate.\n",
    "                            random_state=0,        # Seed\n",
    "                            #missing=None,                 # How are nulls encoded?\n",
    "                            #tree_method='gpu_hist',       # How to train the trees?\n",
    "                            #gpu_id=0                      # With which GPU? \n",
    "                            )\n",
    "     \n",
    "\n",
    "xgb.fit(x_train, y_train)\n",
    "\n",
    "# Prediction \n",
    "y_prediction_xgb = xgb.predict(x_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_xgb = confusion_matrix(y_test, y_prediction_xgb, labels=xgb.classes_)\n",
    "\n",
    "# Display Confusion Matrix Plot\n",
    "disp_xgb = ConfusionMatrixDisplay(confusion_matrix=cm_xgb, display_labels=xgb.classes_)\n",
    "disp_xgb.plot()\n",
    "\n",
    "# Metrics\n",
    "TP_xgb = 42\n",
    "TN_xgb  = 189\n",
    "FP_xgb  = 44\n",
    "FN_xgb  = 25\n",
    "\n",
    "ACC_xgb  = round(((TP_xgb  + TN_xgb ) / (TP_xgb + TN_xgb + FP_xgb + FN_xgb))*100,2)\n",
    "print(f\"XGBoost Accuracy:{ACC_xgb}%\")\n",
    "\n",
    "TPR_xgb  = round((TP_xgb / (TP_xgb + FN_xgb))*100,2) \n",
    "print(f\"XGBoost Sensitivity, TPR:{TPR_logreg}%\")\n",
    "\n",
    "SP_xgb  = round((TN_xgb / (FP_xgb + TN_xgb))*100,2)\n",
    "print(f\"XGBoost Specificity: {SP_xgb}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot variable importance\n",
    "importances = xgb.feature_importances_\n",
    "indices = np.argsort(importances)[::-1] \n",
    "\n",
    "f, ax = plt.subplots(figsize=(3, 8))\n",
    "plt.title(\"Variable Importance - XGBoosting\")\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(y=[df_dummies.iloc[:, :-1].columns[i] for i in indices], x=importances[indices], \n",
    "            label=\"Total\", color=\"b\")\n",
    "ax.set(ylabel=\"Variable\",\n",
    "       xlabel=\"Variable Importance (Entropy)\")\n",
    "sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#OneHotEncoding for categorical variables\n",
    "\n",
    "\n",
    "#Drop 6 columns with P-values less than 0.05 statistical significance\n",
    "\n",
    "df_selected = ger_credit_df.drop(columns=['Number of people being liable to provide maintenance for',\\\n",
    "                      'Present residence since',\\\n",
    "                      'Job',\\\n",
    "                      'Telephone',\\\n",
    "                      'Credit amount',\\\n",
    "                      'Number of existing credits at this bank'\n",
    "                                         ])\n",
    "\n",
    "\n",
    "# Create a list holding 11 categorical variables in the dataset\n",
    "\n",
    "col_cat_names=[\"Status of existing checking account\",\"Credit history\",\"Purpose\",\\\n",
    "\"Savings account/bonds\",\"Present employment since\",\\\n",
    "\"Personal status and sex\",\"Other debtors / guarantors\",\"Property\",\"Other installment plans\",\"Housing\",\"foreign worker\"]\n",
    "\n",
    "# Use one-hot encoding to create dummy variables for 11 categorical variables created and 1 target variable\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, df_selected, col_cat_names):\n",
    "        self.df_selected = df_selected\n",
    "        self.col_cat_names = col_cat_names\n",
    "\n",
    "    def one_hot_encode(self):\n",
    "        for attr in self.col_cat_names:\n",
    "            self.df_selected = self.df_selected.merge(pd.get_dummies(self.df_selected[attr], prefix=attr), left_index=True, right_index=True)\n",
    "            self.df_selected.drop(attr, axis=1, inplace=True)\n",
    "            \n",
    "    def replace_value(self):\n",
    "        risk = {\"Good Risk\": 1, \"Bad Risk\": 0}\n",
    "        self.df_selected[\"Credit Risk\"] = self.df_selected[\"Credit Risk\"].map(risk)\n",
    "        \n",
    "\n",
    "\n",
    "preprocessor = Preprocessor(df_selected, col_cat_names)\n",
    "preprocessor.one_hot_encode()\n",
    "preprocessor.replace_value()\n",
    "df_dummies = preprocessor.df_selected\n",
    "df_dummies\n",
    "# df_selected\n",
    "\n",
    "# ger_credit_df\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Second Data Group for model prediction without \"Credit History\" and \"Foreign Worker\" features\n",
    "\n",
    "df_selected1 = ger_credit_df.drop(columns=['Number of people being liable to provide maintenance for',\\\n",
    "                      'Present residence since','Job', 'Telephone','Credit amount','Number of existing credits at this bank',\\\n",
    "                      'Credit history', 'foreign worker'\n",
    "                                         ])\n",
    "# Second list with names of columns for one-hot encoding of 9 categorical variables for model prediction \n",
    "# without \"Credit History\" and \"Foreign Worker\" features\n",
    "# Final dummy will use 9 categorical variables, and 3 numerical variable and 1 target variable\n",
    "\n",
    "col_cat_names1=[\"Status of existing checking account\",\"Purpose\",\\\n",
    "\"Savings account/bonds\",\"Present employment since\",\\\n",
    "\"Personal status and sex\",\"Other debtors / guarantors\",\"Property\",\"Other installment plans\",\"Housing\"]\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, df_selected1, col_cat_names1):\n",
    "        self.df_selected1 = df_selected1\n",
    "        self.col_cat_names1 = col_cat_names1\n",
    "\n",
    "    def one_hot_encode(self):\n",
    "        for attr in self.col_cat_names1:\n",
    "            self.df_selected1 = self.df_selected1.merge(pd.get_dummies(self.df_selected1[attr], prefix=attr), left_index=True, right_index=True)\n",
    "            self.df_selected1.drop(attr, axis=1, inplace=True)\n",
    "            \n",
    "    def replace_value(self):\n",
    "        risk = {\"Good Risk\": 1, \"Bad Risk\": 0}\n",
    "        self.df_selected1[\"Credit Risk\"] = self.df_selected1[\"Credit Risk\"].map(risk)\n",
    "    \n",
    "\n",
    "preprocessor = Preprocessor(df_selected1, col_cat_names1)\n",
    "preprocessor.one_hot_encode()\n",
    "preprocessor.replace_value()\n",
    "df_dummies1 = preprocessor.df_selected1\n",
    "df_dummies1\n",
    "\n",
    "# df_selected1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Target Variable distribution\n",
    "\n",
    "\n",
    "### OOP Target Data Distribution\n",
    "\n",
    "# Replace \"Good Risk\" and \"Bad Risk\" with \"1\" and \"0\" so it's possible to count occurrence\n",
    "# Use back up data - create a copy from it. \n",
    "\n",
    "df_dist = backup_ger_credit_df.copy()\n",
    "\n",
    "class Replacer:\n",
    "    def __init__(self, df_dist):\n",
    "        self.df_dist = df_dist\n",
    "\n",
    "    def replace_value(self):\n",
    "        risk = {\"Good Risk\": 1, \"Bad Risk\": 0}\n",
    "        self.df_dist[\"Credit Risk\"] = self.df_dist[\"Credit Risk\"].map(risk)\n",
    "        \n",
    "\n",
    "df_replaced = Replacer(df_dist)\n",
    "df_replaced.replace_value()\n",
    "df_dist_replaced = df_replaced.df_dist\n",
    "\n",
    "# Plot graph\n",
    "\n",
    "df_dist_replaced['Credit Risk'].astype(int).plot.hist(color='forestgreen').set_xlabel('Target or dependent variable: 1 or 0');\n",
    "\n",
    "count = df_dist_replaced['Credit Risk'].value_counts()\n",
    "good_risk = count[1]\n",
    "bad_risk = count[0]\n",
    "\n",
    "print(\"There are {} loans repaid on time (Good Risk, Cost Matrix(Risk) =1) and {} loans defaulted (Bad Risk, Cost Matrix(Risk)=0) in the dataset\".format(good_risk, bad_risk))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Spliting dataset into train and test version\n",
    "\n",
    "#Remove Target Variable column, assign resultant dataframe with only indpendent variables to \"x\"\n",
    "x = df_dummies1.drop('Credit Risk', 1).values \n",
    "\n",
    "#Select Target variable column, assign it to \"y\"\n",
    "y = df_dummies1['Credit Risk'].values \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30,random_state=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Scaling the dataset\n",
    "\n",
    "scaled_x = StandardScaler()    \n",
    "x_train = scaled_x.fit_transform(x_train)    \n",
    "x_test = scaled_x.transform(x_test)  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Credit Risk Model Development and Fit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Model Training - Data Group 1\n",
    "l_reg = LogisticRegression (random_state = 0)\n",
    "l_reg.fit(x_train, y_train)\n",
    "\n",
    "# Prediction \n",
    "y_prediction_logreg = l_reg.predict(x_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_logreg = confusion_matrix(y_test, y_prediction_logreg, labels=l_reg.classes_)\n",
    "\n",
    "# Display Confusion Matrix Plot\n",
    "disp_logreg = ConfusionMatrixDisplay(confusion_matrix=cm_logreg, display_labels=l_reg.classes_)\n",
    "disp_logreg.plot()\n",
    "\n",
    "# Metrics\n",
    "TP_logreg = 41\n",
    "TN_logreg  = 175\n",
    "FP_logreg  = 45\n",
    "FN_logreg  = 39\n",
    "\n",
    "ACC_logreg  = round(((TP_logreg  + TN_logreg ) / (TP_logreg + TN_logreg + FP_logreg + FN_logreg))*100,2)\n",
    "print(f\"LogisticRegression Accuracy (%):{ACC_logreg}\")\n",
    "\n",
    "TPR_logreg  = round((TP_logreg / (TP_logreg + FN_logreg))*100,2) \n",
    "print(f\"LogisticRegression Sensitivity, TPR (%):{TPR_logreg}\")\n",
    "\n",
    "SP_logreg  = round((TN_logreg / (FP_logreg + TN_logreg))*100,2)\n",
    "print(f\"LogisticRegression Specificity (%): {SP_logreg}\")\n",
    "      "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Model Training - Data Group 2 - No Credit History or Immigrant tag\n",
    "l_reg = LogisticRegression (random_state = 0)\n",
    "l_reg.fit(x_train, y_train)\n",
    "\n",
    "# Prediction \n",
    "y_prediction_logreg = l_reg.predict(x_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_logreg = confusion_matrix(y_test, y_prediction_logreg, labels=l_reg.classes_)\n",
    "\n",
    "# Display Confusion Matrix Plot\n",
    "disp_logreg = ConfusionMatrixDisplay(confusion_matrix=cm_logreg, display_labels=l_reg.classes_)\n",
    "disp_logreg.plot()\n",
    "\n",
    "# Metrics\n",
    "TP_logreg = 44\n",
    "TN_logreg  = 180\n",
    "FP_logreg  = 42\n",
    "FN_logreg  = 34\n",
    "\n",
    "ACC_logreg  = round(((TP_logreg  + TN_logreg ) / (TP_logreg + TN_logreg + FP_logreg + FN_logreg))*100,2)\n",
    "print(f\"LogisticRegression Accuracy (%):{ACC_logreg}\")\n",
    "\n",
    "TPR_logreg  = round((TP_logreg / (TP_logreg + FN_logreg))*100,2) \n",
    "print(f\"LogisticRegression Sensitivity, TPR (%):{TPR_logreg}\")\n",
    "\n",
    "SP_logreg  = round((TN_logreg / (FP_logreg + TN_logreg))*100,2)\n",
    "print(f\"LogisticRegression Specificity (%): {SP_logreg}\")\n",
    "      "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### XGBoost Classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# XGBoost Model Training - Data Group 1 - complete selected feature\n",
    "\n",
    "xgb = XGBClassifier(max_depth=2,                 # Depth of each tree\n",
    "                            learning_rate=0.1,            # How much to shrink error in each subsequent training. Trade-off with no. estimators.\n",
    "                            n_estimators=50,             # How many trees to use, the more the better, but decrease learning rate if many used.\n",
    "                            verbosity=1,                  # If to show more errors or not.\n",
    "                            objective='binary:logistic',  # Type of target variable.\n",
    "                            booster='gbtree',             # What to boost. Trees in this case.\n",
    "                            n_jobs=2,                    # Parallel jobs to run. Set your processor number.\n",
    "                            gamma=0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n",
    "                            subsample=0.632,              # Subsample ratio. Can set lower\n",
    "                            colsample_bytree=1,           # Subsample ratio of columns when constructing each tree.\n",
    "                            colsample_bylevel=1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest.\n",
    "                            colsample_bynode=1,           # Subsample ratio of columns when constructing each split.\n",
    "                            reg_alpha=1,                  # Regularizer for first fit. alpha = 1, lambda = 0 is LASSO.\n",
    "                            reg_lambda=0,                 # Regularizer for first fit.\n",
    "                            scale_pos_weight=1,           # Balancing of positive and negative weights. G / B\n",
    "                            base_score=0.5,               # Global bias. Set to average of the target rate.\n",
    "                            random_state=0,        # Seed\n",
    "                            #missing=None,                 # How are nulls encoded?\n",
    "                            #tree_method='gpu_hist',       # How to train the trees?\n",
    "                            #gpu_id=0                      # With which GPU? \n",
    "                            )\n",
    "     \n",
    "\n",
    "xgb.fit(x_train, y_train)\n",
    "\n",
    "# Prediction \n",
    "y_prediction_xgb = xgb.predict(x_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_xgb = confusion_matrix(y_test, y_prediction_xgb, labels=xgb.classes_)\n",
    "\n",
    "# Display Confusion Matrix Plot\n",
    "disp_xgb = ConfusionMatrixDisplay(confusion_matrix=cm_xgb, display_labels=xgb.classes_)\n",
    "disp_xgb.plot()\n",
    "\n",
    "# Metrics\n",
    "TP_xgb = 40\n",
    "TN_xgb  = 186\n",
    "FP_xgb  = 46\n",
    "FN_xgb  = 28\n",
    "\n",
    "ACC_xgb  = round(((TP_xgb  + TN_xgb ) / (TP_xgb + TN_xgb + FP_xgb + FN_xgb))*100,2)\n",
    "print(f\"XGBoost Accuracy:{ACC_xgb}%\")\n",
    "\n",
    "TPR_xgb  = round((TP_xgb / (TP_xgb + FN_xgb))*100,2) \n",
    "print(f\"XGBoost Sensitivity, TPR:{TPR_logreg}%\")\n",
    "\n",
    "SP_xgb  = round((TN_xgb / (FP_xgb + TN_xgb))*100,2)\n",
    "print(f\"XGBoost Specificity: {SP_xgb}%\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# XGBoost Model Training - Data Group 2 - No Credit History or Immigrant tag\n",
    "\n",
    "xgb = XGBClassifier(max_depth=2,                 # Depth of each tree\n",
    "                            learning_rate=0.1,            # How much to shrink error in each subsequent training. Trade-off with no. estimators.\n",
    "                            n_estimators=50,             # How many trees to use, the more the better, but decrease learning rate if many used.\n",
    "                            verbosity=1,                  # If to show more errors or not.\n",
    "                            objective='binary:logistic',  # Type of target variable.\n",
    "                            booster='gbtree',             # What to boost. Trees in this case.\n",
    "                            n_jobs=2,                    # Parallel jobs to run. Set your processor number.\n",
    "                            gamma=0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n",
    "                            subsample=0.632,              # Subsample ratio. Can set lower\n",
    "                            colsample_bytree=1,           # Subsample ratio of columns when constructing each tree.\n",
    "                            colsample_bylevel=1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest.\n",
    "                            colsample_bynode=1,           # Subsample ratio of columns when constructing each split.\n",
    "                            reg_alpha=1,                  # Regularizer for first fit. alpha = 1, lambda = 0 is LASSO.\n",
    "                            reg_lambda=0,                 # Regularizer for first fit.\n",
    "                            scale_pos_weight=1,           # Balancing of positive and negative weights. G / B\n",
    "                            base_score=0.5,               # Global bias. Set to average of the target rate.\n",
    "                            random_state=0,        # Seed\n",
    "                            #missing=None,                 # How are nulls encoded?\n",
    "                            #tree_method='gpu_hist',       # How to train the trees?\n",
    "                            #gpu_id=0                      # With which GPU? \n",
    "                            )\n",
    "     \n",
    "\n",
    "xgb.fit(x_train, y_train)\n",
    "\n",
    "# Prediction \n",
    "y_prediction_xgb = xgb.predict(x_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_xgb = confusion_matrix(y_test, y_prediction_xgb, labels=xgb.classes_)\n",
    "\n",
    "# Display Confusion Matrix Plot\n",
    "disp_xgb = ConfusionMatrixDisplay(confusion_matrix=cm_xgb, display_labels=xgb.classes_)\n",
    "disp_xgb.plot()\n",
    "\n",
    "# Metrics\n",
    "TP_xgb = 42\n",
    "TN_xgb  = 189\n",
    "FP_xgb  = 44\n",
    "FN_xgb  = 25\n",
    "\n",
    "ACC_xgb  = round(((TP_xgb  + TN_xgb ) / (TP_xgb + TN_xgb + FP_xgb + FN_xgb))*100,2)\n",
    "print(f\"XGBoost Accuracy:{ACC_xgb}%\")\n",
    "\n",
    "TPR_xgb  = round((TP_xgb / (TP_xgb + FN_xgb))*100,2) \n",
    "print(f\"XGBoost Sensitivity, TPR:{TPR_logreg}%\")\n",
    "\n",
    "SP_xgb  = round((TN_xgb / (FP_xgb + TN_xgb))*100,2)\n",
    "print(f\"XGBoost Specificity: {SP_xgb}%\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot variable importance\n",
    "importances = xgb.feature_importances_\n",
    "indices = np.argsort(importances)[::-1] \n",
    "\n",
    "f, ax = plt.subplots(figsize=(3, 8))\n",
    "plt.title(\"Variable Importance - XGBoosting\")\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(y=[df_dummies.iloc[:, :-1].columns[i] for i in indices], x=importances[indices], \n",
    "            label=\"Total\", color=\"b\")\n",
    "ax.set(ylabel=\"Variable\",\n",
    "       xlabel=\"Variable Importance (Entropy)\")\n",
    "sns.despine(left=True, bottom=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Support Vector Machines (SVM)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36m<cell line: 13>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      8\u001B[0m y_pred_svm \u001B[38;5;241m=\u001B[39m model_svm\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Metrics\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m cm_svm \u001B[38;5;241m=\u001B[39m confusion_matrix(y_test, y_pred, labels\u001B[38;5;241m=\u001B[39m\u001B[43msvm\u001B[49m\u001B[38;5;241m.\u001B[39mclasses_)\n\u001B[0;32m     14\u001B[0m acc_svm \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mround\u001B[39m(accuracy_score(y_test,y_pred)\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m,\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m     15\u001B[0m class_met_svm \u001B[38;5;241m=\u001B[39m classification_report(y_test,y_pred_svm)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'svm' is not defined"
     ]
    }
   ],
   "source": [
    "# Second Data Group for model prediction without \"Credit History\" and \"Foreign Worker\" features\n",
    "\n",
    "df_selected1 = ger_credit_df.drop(columns=['Number of people being liable to provide maintenance for',\\\n",
    "                      'Present residence since','Job', 'Telephone','Credit amount','Number of existing credits at this bank',\\\n",
    "                      'Credit history', 'foreign worker'\n",
    "                                         ])\n",
    "# Second list with names of columns for one-hot encoding of 9 categorical variables for model prediction \n",
    "# without \"Credit History\" and \"Foreign Worker\" features\n",
    "# Final dummy will use 9 categorical variables, and 3 numerical variable and 1 target variable\n",
    "\n",
    "col_cat_names1=[\"Status of existing checking account\",\"Purpose\",\\\n",
    "\"Savings account/bonds\",\"Present employment since\",\\\n",
    "\"Personal status and sex\",\"Other debtors / guarantors\",\"Property\",\"Other installment plans\",\"Housing\"]\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, df_selected1, col_cat_names1):\n",
    "        self.df_selected1 = df_selected1\n",
    "        self.col_cat_names1 = col_cat_names1\n",
    "\n",
    "    def one_hot_encode(self):\n",
    "        for attr in self.col_cat_names1:\n",
    "            self.df_selected1 = self.df_selected1.merge(pd.get_dummies(self.df_selected1[attr], prefix=attr), left_index=True, right_index=True)\n",
    "            self.df_selected1.drop(attr, axis=1, inplace=True)\n",
    "            \n",
    "    def replace_value(self):\n",
    "        risk = {\"Good Risk\": 1, \"Bad Risk\": 0}\n",
    "        self.df_selected1[\"Credit Risk\"] = self.df_selected1[\"Credit Risk\"].map(risk)\n",
    "    \n",
    "\n",
    "preprocessor = Preprocessor(df_selected1, col_cat_names1)\n",
    "preprocessor.one_hot_encode()\n",
    "preprocessor.replace_value()\n",
    "df_dummies1 = preprocessor.df_selected1\n",
    "df_dummies1\n",
    "\n",
    "# df_selected1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Target Variable distribution\n",
    "\n",
    "\n",
    "### OOP Target Data Distribution\n",
    "\n",
    "# Replace \"Good Risk\" and \"Bad Risk\" with \"1\" and \"0\" so it's possible to count occurrence\n",
    "# Use back up data - create a copy from it. \n",
    "\n",
    "df_dist = backup_ger_credit_df.copy()\n",
    "\n",
    "class Replacer:\n",
    "    def __init__(self, df_dist):\n",
    "        self.df_dist = df_dist\n",
    "\n",
    "    def replace_value(self):\n",
    "        risk = {\"Good Risk\": 1, \"Bad Risk\": 0}\n",
    "        self.df_dist[\"Credit Risk\"] = self.df_dist[\"Credit Risk\"].map(risk)\n",
    "        \n",
    "\n",
    "df_replaced = Replacer(df_dist)\n",
    "df_replaced.replace_value()\n",
    "df_dist_replaced = df_replaced.df_dist\n",
    "\n",
    "# Plot graph\n",
    "\n",
    "df_dist_replaced['Credit Risk'].astype(int).plot.hist(color='forestgreen').set_xlabel('Target or dependent variable: 1 or 0');\n",
    "\n",
    "count = df_dist_replaced['Credit Risk'].value_counts()\n",
    "good_risk = count[1]\n",
    "bad_risk = count[0]\n",
    "\n",
    "print(\"There are {} loans repaid on time (Good Risk, Cost Matrix(Risk) =1) and {} loans defaulted (Bad Risk, Cost Matrix(Risk)=0) in the dataset\".format(good_risk, bad_risk))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Spliting dataset into train and test version\n",
    "\n",
    "#Remove Target Variable column, assign resultant dataframe with only indpendent variables to \"x\"\n",
    "x = df_dummies1.drop('Credit Risk', 1).values \n",
    "\n",
    "#Select Target variable column, assign it to \"y\"\n",
    "y = df_dummies1['Credit Risk'].values \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Scaling the dataset\n",
    "\n",
    "scaled_x = StandardScaler()    \n",
    "x_train = scaled_x.fit_transform(x_train)    \n",
    "x_test = scaled_x.transform(x_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Credit Risk Model Development and Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Model Training - Data Group 1\n",
    "l_reg = LogisticRegression (random_state = 0)\n",
    "l_reg.fit(x_train, y_train)\n",
    "\n",
    "# Prediction \n",
    "y_prediction_logreg = l_reg.predict(x_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_logreg = confusion_matrix(y_test, y_prediction_logreg, labels=l_reg.classes_)\n",
    "\n",
    "# Display Confusion Matrix Plot\n",
    "disp_logreg = ConfusionMatrixDisplay(confusion_matrix=cm_logreg, display_labels=l_reg.classes_)\n",
    "disp_logreg.plot()\n",
    "\n",
    "# Metrics\n",
    "TP_logreg = 41\n",
    "TN_logreg  = 175\n",
    "FP_logreg  = 45\n",
    "FN_logreg  = 39\n",
    "\n",
    "ACC_logreg  = round(((TP_logreg  + TN_logreg ) / (TP_logreg + TN_logreg + FP_logreg + FN_logreg))*100,2)\n",
    "print(f\"LogisticRegression Accuracy (%):{ACC_logreg}\")\n",
    "\n",
    "TPR_logreg  = round((TP_logreg / (TP_logreg + FN_logreg))*100,2) \n",
    "print(f\"LogisticRegression Sensitivity, TPR (%):{TPR_logreg}\")\n",
    "\n",
    "SP_logreg  = round((TN_logreg / (FP_logreg + TN_logreg))*100,2)\n",
    "print(f\"LogisticRegression Specificity (%): {SP_logreg}\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Model Training - Data Group 2 - No Credit History or Immigrant tag\n",
    "l_reg = LogisticRegression (random_state = 0)\n",
    "l_reg.fit(x_train, y_train)\n",
    "\n",
    "# Prediction \n",
    "y_prediction_logreg = l_reg.predict(x_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_logreg = confusion_matrix(y_test, y_prediction_logreg, labels=l_reg.classes_)\n",
    "\n",
    "# Display Confusion Matrix Plot\n",
    "disp_logreg = ConfusionMatrixDisplay(confusion_matrix=cm_logreg, display_labels=l_reg.classes_)\n",
    "disp_logreg.plot()\n",
    "\n",
    "# Metrics\n",
    "TP_logreg = 44\n",
    "TN_logreg  = 180\n",
    "FP_logreg  = 42\n",
    "FN_logreg  = 34\n",
    "\n",
    "ACC_logreg  = round(((TP_logreg  + TN_logreg ) / (TP_logreg + TN_logreg + FP_logreg + FN_logreg))*100,2)\n",
    "print(f\"LogisticRegression Accuracy (%):{ACC_logreg}\")\n",
    "\n",
    "TPR_logreg  = round((TP_logreg / (TP_logreg + FN_logreg))*100,2) \n",
    "print(f\"LogisticRegression Sensitivity, TPR (%):{TPR_logreg}\")\n",
    "\n",
    "SP_logreg  = round((TN_logreg / (FP_logreg + TN_logreg))*100,2)\n",
    "print(f\"LogisticRegression Specificity (%): {SP_logreg}\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# XGBoost Model Training - Data Group 1 - complete selected feature\n",
    "\n",
    "xgb = XGBClassifier(max_depth=2,                 # Depth of each tree\n",
    "                            learning_rate=0.1,            # How much to shrink error in each subsequent training. Trade-off with no. estimators.\n",
    "                            n_estimators=50,             # How many trees to use, the more the better, but decrease learning rate if many used.\n",
    "                            verbosity=1,                  # If to show more errors or not.\n",
    "                            objective='binary:logistic',  # Type of target variable.\n",
    "                            booster='gbtree',             # What to boost. Trees in this case.\n",
    "                            n_jobs=2,                    # Parallel jobs to run. Set your processor number.\n",
    "                            gamma=0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n",
    "                            subsample=0.632,              # Subsample ratio. Can set lower\n",
    "                            colsample_bytree=1,           # Subsample ratio of columns when constructing each tree.\n",
    "                            colsample_bylevel=1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest.\n",
    "                            colsample_bynode=1,           # Subsample ratio of columns when constructing each split.\n",
    "                            reg_alpha=1,                  # Regularizer for first fit. alpha = 1, lambda = 0 is LASSO.\n",
    "                            reg_lambda=0,                 # Regularizer for first fit.\n",
    "                            scale_pos_weight=1,           # Balancing of positive and negative weights. G / B\n",
    "                            base_score=0.5,               # Global bias. Set to average of the target rate.\n",
    "                            random_state=0,        # Seed\n",
    "                            #missing=None,                 # How are nulls encoded?\n",
    "                            #tree_method='gpu_hist',       # How to train the trees?\n",
    "                            #gpu_id=0                      # With which GPU? \n",
    "                            )\n",
    "     \n",
    "\n",
    "xgb.fit(x_train, y_train)\n",
    "\n",
    "# Prediction \n",
    "y_prediction_xgb = xgb.predict(x_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_xgb = confusion_matrix(y_test, y_prediction_xgb, labels=xgb.classes_)\n",
    "\n",
    "# Display Confusion Matrix Plot\n",
    "disp_xgb = ConfusionMatrixDisplay(confusion_matrix=cm_xgb, display_labels=xgb.classes_)\n",
    "disp_xgb.plot()\n",
    "\n",
    "# Metrics\n",
    "TP_xgb = 40\n",
    "TN_xgb  = 186\n",
    "FP_xgb  = 46\n",
    "FN_xgb  = 28\n",
    "\n",
    "ACC_xgb  = round(((TP_xgb  + TN_xgb ) / (TP_xgb + TN_xgb + FP_xgb + FN_xgb))*100,2)\n",
    "print(f\"XGBoost Accuracy:{ACC_xgb}%\")\n",
    "\n",
    "TPR_xgb  = round((TP_xgb / (TP_xgb + FN_xgb))*100,2) \n",
    "print(f\"XGBoost Sensitivity, TPR:{TPR_logreg}%\")\n",
    "\n",
    "SP_xgb  = round((TN_xgb / (FP_xgb + TN_xgb))*100,2)\n",
    "print(f\"XGBoost Specificity: {SP_xgb}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# XGBoost Model Training - Data Group 2 - No Credit History or Immigrant tag\n",
    "\n",
    "xgb = XGBClassifier(max_depth=2,                 # Depth of each tree\n",
    "                            learning_rate=0.1,            # How much to shrink error in each subsequent training. Trade-off with no. estimators.\n",
    "                            n_estimators=50,             # How many trees to use, the more the better, but decrease learning rate if many used.\n",
    "                            verbosity=1,                  # If to show more errors or not.\n",
    "                            objective='binary:logistic',  # Type of target variable.\n",
    "                            booster='gbtree',             # What to boost. Trees in this case.\n",
    "                            n_jobs=2,                    # Parallel jobs to run. Set your processor number.\n",
    "                            gamma=0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n",
    "                            subsample=0.632,              # Subsample ratio. Can set lower\n",
    "                            colsample_bytree=1,           # Subsample ratio of columns when constructing each tree.\n",
    "                            colsample_bylevel=1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest.\n",
    "                            colsample_bynode=1,           # Subsample ratio of columns when constructing each split.\n",
    "                            reg_alpha=1,                  # Regularizer for first fit. alpha = 1, lambda = 0 is LASSO.\n",
    "                            reg_lambda=0,                 # Regularizer for first fit.\n",
    "                            scale_pos_weight=1,           # Balancing of positive and negative weights. G / B\n",
    "                            base_score=0.5,               # Global bias. Set to average of the target rate.\n",
    "                            random_state=0,        # Seed\n",
    "                            #missing=None,                 # How are nulls encoded?\n",
    "                            #tree_method='gpu_hist',       # How to train the trees?\n",
    "                            #gpu_id=0                      # With which GPU? \n",
    "                            )\n",
    "     \n",
    "\n",
    "xgb.fit(x_train, y_train)\n",
    "\n",
    "# Prediction \n",
    "y_prediction_xgb = xgb.predict(x_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_xgb = confusion_matrix(y_test, y_prediction_xgb, labels=xgb.classes_)\n",
    "\n",
    "# Display Confusion Matrix Plot\n",
    "disp_xgb = ConfusionMatrixDisplay(confusion_matrix=cm_xgb, display_labels=xgb.classes_)\n",
    "disp_xgb.plot()\n",
    "\n",
    "# Metrics\n",
    "TP_xgb = 42\n",
    "TN_xgb  = 189\n",
    "FP_xgb  = 44\n",
    "FN_xgb  = 25\n",
    "\n",
    "ACC_xgb  = round(((TP_xgb  + TN_xgb ) / (TP_xgb + TN_xgb + FP_xgb + FN_xgb))*100,2)\n",
    "print(f\"XGBoost Accuracy:{ACC_xgb}%\")\n",
    "\n",
    "TPR_xgb  = round((TP_xgb / (TP_xgb + FN_xgb))*100,2) \n",
    "print(f\"XGBoost Sensitivity, TPR:{TPR_logreg}%\")\n",
    "\n",
    "SP_xgb  = round((TN_xgb / (FP_xgb + TN_xgb))*100,2)\n",
    "print(f\"XGBoost Specificity: {SP_xgb}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot variable importance\n",
    "importances = xgb.feature_importances_\n",
    "indices = np.argsort(importances)[::-1] \n",
    "\n",
    "f, ax = plt.subplots(figsize=(3, 8))\n",
    "plt.title(\"Variable Importance - XGBoosting\")\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(y=[df_dummies.iloc[:, :-1].columns[i] for i in indices], x=importances[indices], \n",
    "            label=\"Total\", color=\"b\")\n",
    "ax.set(ylabel=\"Variable\",\n",
    "       xlabel=\"Variable Importance (Entropy)\")\n",
    "sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Support Vector Machines\n",
    "\n",
    "# Train the model\n",
    "model_svm = SVC(kernel = 'linear', random_state = 0, probability=True)  \n",
    "model_svm.fit(X_train, y_train) \n",
    "\n",
    "# Model Prediction\n",
    "y_pred_svm = model_svm.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Metrics\n",
    "cm_svm = confusion_matrix(y_test, y_pred, labels=svm.classes_)\n",
    "acc_svm = round(accuracy_score(y_test,y_pred)*100,2)\n",
    "class_met_svm = classification_report(y_test,y_pred_svm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"XGBoost Accuracy(%): {acc_xgboost}\")\n",
    "print(f\"SVM Accuracy(%): {acc_svm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Precision, Recall and F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"---------- XGBoost Classification Report----------\")\n",
    "print(\"\")\n",
    "print(class_met_xgboost)\n",
    "\n",
    "\n",
    "print(\"----------SVM Classification Report-----------\")\n",
    "print(\"\")\n",
    "print(class_met_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_svm, display_labels=model_svm.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TP = 35\n",
    "TN = 188\n",
    "FP = 51\n",
    "FN = 26\n",
    "ACC = round(((TP + TN) / (TP + TN + FP + FN))*100,2)\n",
    "print(f\"Accuracy (%):{ACC}\")\n",
    "\n",
    "TPR = round((TP / (TP + FN))*100,2) \n",
    "print(f\"Sensitivity, TPR (%):{TPR}\")\n",
    "\n",
    "SP = round((TN / (FP + TN))*100,2)\n",
    "\n",
    "print(f\"SVM Specificity (%): {SP}\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# SVM ROC\n",
    "\n",
    "y_pred_svmroc = model_svm.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_svmroc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('SVM ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "# XGBoost ROC\n",
    "\n",
    "y_pred_xgbroc = model_xgboost.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_xgbroc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('XGBoost ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TP = 58\n",
    "TN = 172\n",
    "FP = 28\n",
    "FN = 42\n",
    "ACC = round(((TP + TN) / (TP + TN + FP + FN))*100,2)\n",
    "print(f\"XGBoost Accuracy (%):{ACC}\")\n",
    "\n",
    "TPR = round((TP / (TP + FN))*100,2) \n",
    "print(f\"XGBoost Sensitivity, TPR (%):{TPR}\")\n",
    "\n",
    "SP = round((TN / (FP + TN))*100,2)\n",
    "\n",
    "print(f\"XGBoost Specificity (%): {SP}\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Support Vector Machines\n",
    "\n",
    "# Train the model\n",
    "model_svm = SVC(kernel = 'linear', random_state = 0, probability=True)  \n",
    "model_svm.fit(X_train, y_train) \n",
    "\n",
    "# Model Prediction\n",
    "y_pred_svm = model_svm.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Metrics\n",
    "cm_svm = confusion_matrix(y_test, y_pred, labels=svm.classes_)\n",
    "acc_svm = round(accuracy_score(y_test,y_pred)*100,2)\n",
    "class_met_svm = classification_report(y_test,y_pred_svm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"XGBoost Accuracy(%): {acc_xgboost}\")\n",
    "print(f\"SVM Accuracy(%): {acc_svm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Precision, Recall and F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"---------- XGBoost Classification Report----------\")\n",
    "print(\"\")\n",
    "print(class_met_xgboost)\n",
    "\n",
    "\n",
    "print(\"----------SVM Classification Report-----------\")\n",
    "print(\"\")\n",
    "print(class_met_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_svm, display_labels=model_svm.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TP = 35\n",
    "TN = 188\n",
    "FP = 51\n",
    "FN = 26\n",
    "ACC = round(((TP + TN) / (TP + TN + FP + FN))*100,2)\n",
    "print(f\"Accuracy (%):{ACC}\")\n",
    "\n",
    "TPR = round((TP / (TP + FN))*100,2) \n",
    "print(f\"Sensitivity, TPR (%):{TPR}\")\n",
    "\n",
    "SP = round((TN / (FP + TN))*100,2)\n",
    "\n",
    "print(f\"SVM Specificity (%): {SP}\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# SVM ROC\n",
    "\n",
    "y_pred_svmroc = model_svm.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_svmroc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('SVM ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "# XGBoost ROC\n",
    "\n",
    "y_pred_xgbroc = model_xgboost.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_xgbroc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('XGBoost ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# GUI to avoid blackbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}